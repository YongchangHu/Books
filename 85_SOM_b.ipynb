{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "85 SOM b",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YongchangHu/Books/blob/master/85_SOM_b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woJOFpeEdR8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys, os,cv2\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.misc import imread,imresize\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from skimage.transform import resize\n",
        "from imgaug import augmenters as iaa\n",
        "import imgaug as ia\n",
        "from skimage.color import rgba2rgb\n",
        "\n",
        "old_v = tf.logging.get_verbosity()\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "plt.style.use('seaborn-white')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "np.random.seed(6278)\n",
        "tf.set_random_seed(6728)\n",
        "ia.seed(6278)\n",
        "\n",
        "def tf_elu(x): return tf.nn.elu(x)\n",
        "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
        "\n",
        "def tf_tanh(x): return tf.nn.tanh(x)\n",
        "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
        "\n",
        "def tf_sigmoid(x): return tf.nn.sigmoid(x) \n",
        "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
        "\n",
        "def tf_atan(x): return tf.atan(x)\n",
        "def d_tf_atan(x): return 1.0/(1.0 + x**2)\n",
        "\n",
        "def tf_iden(x): return x\n",
        "def d_tf_iden(x): return 1.0\n",
        "\n",
        "def tf_softmax(x): return tf.nn.softmax(x)\n",
        "\n",
        "# code from: https://github.com/tensorflow/tensorflow/issues/8246\n",
        "def tf_repeat(tensor, repeats):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    input: A Tensor. 1-D or higher.\n",
        "    repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\n",
        "    Returns:\n",
        "    \n",
        "    A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\n",
        "    \"\"\"\n",
        "    expanded_tensor = tf.expand_dims(tensor, -1)\n",
        "    multiples = [1] + repeats\n",
        "    tiled_tensor = tf.tile(expanded_tensor, multiples = multiples)\n",
        "    repeated_tesnor = tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\n",
        "    return repeated_tesnor\n",
        "\n",
        "# ================= VIZ =================\n",
        "# Def: Simple funciton to view the histogram of weights\n",
        "def show_hist_of_weigt(all_weight_list,status='before'):\n",
        "    fig = plt.figure()\n",
        "    weight_index = 0\n",
        "\n",
        "    for i in range(1,1+int(len(all_weight_list)//3)):\n",
        "        ax = fig.add_subplot(1,4,i)\n",
        "        ax.grid(False)\n",
        "        temp_weight_list = all_weight_list[weight_index:weight_index+3]\n",
        "        for temp_index in range(len(temp_weight_list)):\n",
        "            current_flat = temp_weight_list[temp_index].flatten()\n",
        "            ax.hist(current_flat,histtype='step',bins='auto',label=str(temp_index+weight_index))\n",
        "            ax.legend()\n",
        "        ax.set_title('From Layer : '+str(weight_index+1)+' to '+str(weight_index+3))\n",
        "        weight_index = weight_index + 3\n",
        "    plt.savefig('viz/weights_'+str(status)+\"_training.png\")\n",
        "    plt.close('all')\n",
        "\n",
        "# Def: Simple function to show 9 image with different channels\n",
        "def show_9_images(image,layer_num,image_num,channel_increase=3,alpha=None,gt=None,predict=None):\n",
        "    image = (image-image.min())/(image.max()-image.min())\n",
        "    fig = plt.figure()\n",
        "    color_channel = 0\n",
        "    limit = 10\n",
        "    if alpha: limit = len(gt)\n",
        "    for i in range(1,limit):\n",
        "        ax = fig.add_subplot(3,3,i)\n",
        "        ax.grid(False)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        if alpha:\n",
        "            ax.set_title(\"GT: \"+str(gt[i-1])+\" Predict: \"+str(predict[i-1]))\n",
        "        else:\n",
        "            ax.set_title(\"Channel : \" + str(color_channel) + \" : \" + str(color_channel+channel_increase-1))\n",
        "        ax.imshow(np.squeeze(image[:,:,color_channel:color_channel+channel_increase]))\n",
        "        color_channel = color_channel + channel_increase\n",
        "    \n",
        "    if alpha:\n",
        "        plt.savefig('viz/z_'+str(alpha) + \"_alpha_image.png\")\n",
        "    else:\n",
        "        plt.savefig('viz/'+str(layer_num) + \"_layer_\"+str(image_num)+\"_image.png\")\n",
        "    plt.close('all')\n",
        "# ================= VIZ =================\n",
        "\n",
        "# ================= LAYER CLASSES =================\n",
        "class CNN():\n",
        "    \n",
        "    def __init__(self,k,inc,out,act=tf_elu,d_act=d_tf_elu):\n",
        "        self.w = tf.Variable(tf.random_normal([k,k,inc,out],stddev=0.05))\n",
        "        self.m,self.v_prev = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
        "        self.act,self.d_act = act,d_act\n",
        "\n",
        "    def getw(self): return self.w\n",
        "\n",
        "    def feedforward(self,input,stride=1,padding='SAME'):\n",
        "        self.input  = input\n",
        "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA \n",
        "\n",
        "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
        "        grad_part_1 = gradient \n",
        "        grad_part_2 = self.d_act(self.layer) \n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = grad_part_1 * grad_part_2\n",
        "\n",
        "        grad = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = self.w.shape,out_backprop = grad_middle,\n",
        "            strides=[1,stride,stride,1],padding=padding\n",
        "        )\n",
        "\n",
        "        grad_pass = tf.nn.conv2d_backprop_input(input_sizes = [batch_size] + list(grad_part_3.shape[1:]),filter= self.w,out_backprop = grad_middle,\n",
        "            strides=[1,stride,stride,1],padding=padding\n",
        "        )\n",
        "\n",
        "        update_w = []\n",
        "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
        "        update_w.append(tf.assign( self.v_prev,self.v_prev*beta2 + (1-beta2) * (grad ** 2)   ))\n",
        "        m_hat = self.m / (1-beta1)\n",
        "        v_hat = self.v_prev / (1-beta2)\n",
        "        adam_middel = learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
        "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat)  )))         \n",
        "\n",
        "        return grad_pass,update_w \n",
        "\n",
        "class CNN_Trans():\n",
        "    \n",
        "    def __init__(self,k,inc,out,act=tf_elu,d_act=d_tf_elu):\n",
        "        self.w = tf.Variable(tf.random_normal([k,k,inc,out],stddev=0.05))\n",
        "        self.m,self.v_prev = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
        "        self.act,self.d_act = act,d_act\n",
        "\n",
        "    def getw(self): return self.w\n",
        "\n",
        "    def feedforward(self,input,stride=1,padding='SAME'):\n",
        "        self.input  = input\n",
        "        output_shape2 = self.input.shape[2].value * stride\n",
        "        self.layer  = tf.nn.conv2d_transpose(\n",
        "            input,self.w,output_shape=[batch_size,output_shape2,output_shape2,self.w.shape[2].value],\n",
        "            strides=[1,stride,stride,1],padding=padding) \n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA \n",
        "\n",
        "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
        "        grad_part_1 = gradient \n",
        "        grad_part_2 = self.d_act(self.layer) \n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = grad_part_1 * grad_part_2\n",
        "\n",
        "        grad = tf.nn.conv2d_backprop_filter(input = grad_middle,\n",
        "            filter_sizes = self.w.shape,out_backprop = grad_part_3,\n",
        "            strides=[1,stride,stride,1],padding=padding\n",
        "        )\n",
        "\n",
        "        grad_pass = tf.nn.conv2d(\n",
        "            input=grad_middle,filter = self.w,strides=[1,stride,stride,1],padding=padding\n",
        "        )\n",
        "        \n",
        "        update_w = []\n",
        "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
        "        update_w.append(tf.assign( self.v_prev,self.v_prev*beta2 + (1-beta2) * (grad ** 2)   ))\n",
        "        m_hat = self.m / (1-beta1)\n",
        "        v_hat = self.v_prev / (1-beta2)\n",
        "        adam_middel = learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
        "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat)  )))         \n",
        "\n",
        "        return grad_pass,update_w \n",
        "\n",
        "class FNN():\n",
        "    \n",
        "    def __init__(self,input_dim,hidden_dim,act,d_act):\n",
        "        self.w = tf.Variable(tf.random_normal([input_dim,hidden_dim], stddev=0.05,seed=2))\n",
        "        self.m,self.v_prev = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
        "        self.v_hat_prev = tf.Variable(tf.zeros_like(self.w))\n",
        "        self.act,self.d_act = act,d_act\n",
        "\n",
        "    def feedforward(self,input=None):\n",
        "        self.input = input\n",
        "        self.layer = tf.matmul(input,self.w)\n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "    def backprop(self,gradient=None):\n",
        "        grad_part_1 = gradient \n",
        "        grad_part_2 = self.d_act(self.layer) \n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = grad_part_1 * grad_part_2\n",
        "        grad = tf.matmul(tf.transpose(grad_part_3),grad_middle)\n",
        "        grad_pass = tf.matmul(tf.multiply(grad_part_1,grad_part_2),tf.transpose(self.w))\n",
        "\n",
        "        update_w = []\n",
        "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
        "        update_w.append(tf.assign( self.v_prev,self.v_prev*beta2 + (1-beta2) * (grad ** 2)   ))\n",
        "        m_hat = self.m / (1-beta1)\n",
        "        v_hat = self.v_prev / (1-beta2)\n",
        "        adam_middel = learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
        "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat)  )))     \n",
        "\n",
        "        return grad_pass,update_w   \n",
        "\n",
        "class SOM_Layer(): \n",
        "\n",
        "    def __init__(self,m,n,dim,num_epoch,learning_rate_som = 0.04,radius_factor = 1.1,gaussian_std=0.5):\n",
        "        \n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.dim = dim\n",
        "        self.gaussian_std = gaussian_std\n",
        "        self.num_epoch = num_epoch\n",
        "        self.map = tf.Variable(tf.random_uniform(shape=[m*n,dim],minval=0.0,maxval=1.0,seed=2))\n",
        "\n",
        "        self.location_vects = tf.constant(np.array(list(self._neuron_locations(m, n))))\n",
        "        self.alpha = learning_rate_som\n",
        "        self.sigma = max(m,n)*1.1\n",
        "\n",
        "    def _neuron_locations(self, m, n):\n",
        "        \"\"\"\n",
        "        Yields one by one the 2-D locations of the individual neurons in the SOM.\n",
        "        \"\"\"\n",
        "        # Nested iterations over both dimensions to generate all 2-D locations in the map\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                yield np.array([i, j])\n",
        "\n",
        "    def getmap(self): return self.map\n",
        "    def getlocation(self): return self.bmu_locs\n",
        "\n",
        "    def feedforward(self,input):\n",
        "        self.input = input\n",
        "        self.grad_pass = tf.pow(tf.subtract(tf.expand_dims(self.map, axis=0),tf.expand_dims(self.input, axis=1)), 2)\n",
        "        self.squared_distance = tf.reduce_sum(self.grad_pass, 2)\n",
        "        self.bmu_indices = tf.argmin(self.squared_distance, axis=1)\n",
        "        self.bmu_locs = tf.reshape(tf.gather(self.location_vects, self.bmu_indices), [-1, 2])\n",
        "\n",
        "    def backprop(self,iter,num_epoch):\n",
        "\n",
        "        # Update the weigths \n",
        "        radius = tf.subtract(self.sigma,\n",
        "                                tf.multiply(iter,\n",
        "                                            tf.divide(tf.cast(tf.subtract(self.alpha, 1),tf.float32),\n",
        "                                                    tf.cast(tf.subtract(num_epoch, 1),tf.float32))))\n",
        "\n",
        "        alpha = tf.subtract(self.alpha,\n",
        "                            tf.multiply(iter,\n",
        "                                            tf.divide(tf.cast(tf.subtract(self.alpha, 1),tf.float32),\n",
        "                                                      tf.cast(tf.subtract(num_epoch, 1),tf.float32))))\n",
        "\n",
        "        self.bmu_distance_squares = tf.reduce_sum(\n",
        "                tf.pow(tf.subtract(\n",
        "                    tf.expand_dims(self.location_vects, axis=0),\n",
        "                    tf.expand_dims(self.bmu_locs, axis=1)), 2), \n",
        "            2)\n",
        "\n",
        "        self.neighbourhood_func = tf.exp(tf.divide(tf.negative(tf.cast(\n",
        "                self.bmu_distance_squares, \"float32\")), tf.multiply(\n",
        "                tf.square(tf.multiply(radius, self.gaussian_std)), 2)))\n",
        "\n",
        "        self.learning_rate_op = tf.multiply(self.neighbourhood_func, alpha)\n",
        "        \n",
        "        self.numerator = tf.reduce_sum(\n",
        "            tf.multiply(tf.expand_dims(self.learning_rate_op, axis=-1),\n",
        "            tf.expand_dims(self.input, axis=1)), axis=0)\n",
        "\n",
        "        self.denominator = tf.expand_dims(\n",
        "            tf.reduce_sum(self.learning_rate_op,axis=0) + float(1e-20), axis=-1)\n",
        "\n",
        "        self.new_weights = tf.div(self.numerator, self.denominator)\n",
        "        self.update = [tf.assign(self.map, self.new_weights)]\n",
        "\n",
        "        return self.update,tf.reduce_mean(self.grad_pass,1)\n",
        "\n",
        "# ================= LAYER CLASSES =================\n",
        "\n",
        "# data\n",
        "mnist = input_data.read_data_sets('../../Dataset/MNIST/', one_hot=True)\n",
        "train, test = tf.keras.datasets.mnist.load_data()\n",
        "x_data, train_label, y_data, test_label = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
        "\n",
        "train_batch = x_data/1.0\n",
        "test_batch = y_data/1.0\n",
        "train_batch = train_batch[:100,:]\n",
        "train_label = train_label[:100,:]\n",
        "test_batch = test_batch[:50,:]\n",
        "test_label = test_label[:50,:]\n",
        "\n",
        "# print out the data shape\n",
        "print(train_batch.shape)\n",
        "print(train_label.shape)\n",
        "print(test_batch.shape)\n",
        "print(test_label.shape)\n",
        "\n",
        "# hyper parameter \n",
        "num_epoch = 100\n",
        "batch_size = 100\n",
        "\n",
        "map_width_height  = 30\n",
        "map_dim = 256\n",
        "\n",
        "learning_rate = 0.0000001\n",
        "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
        "\n",
        "# define class here\n",
        "el1 = FNN(784,512,act=tf_atan,d_act=d_tf_atan)\n",
        "el2 = FNN(512,256,act=tf_atan,d_act=d_tf_atan)\n",
        "SOM_layer = SOM_Layer(map_width_height,map_width_height,map_dim,\n",
        "num_epoch = num_epoch,learning_rate_som=0.8,radius_factor=1.1,gaussian_std = 0.08 )\n",
        "\n",
        "# graph\n",
        "x = tf.placeholder(shape=[None,784],dtype=tf.float32,name=\"input\")\n",
        "current_iter = tf.placeholder(shape=[],dtype=tf.float32)\n",
        "\n",
        "# encoder\n",
        "elayer1 = el1.feedforward(x)\n",
        "elayer2 = el2.feedforward(elayer1)\n",
        "SOM_layer.feedforward(elayer2)\n",
        "\n",
        "SOM_Update,SOM_grad = SOM_layer.backprop(current_iter,num_epoch)\n",
        "egrad2,egrad2_up = el2.backprop(SOM_grad)\n",
        "egrad1,egrad1_up = el1.backprop(egrad2)\n",
        "grad_update = SOM_Update + egrad2_up + egrad1_up\n",
        "\n",
        "# sess\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # start the training\n",
        "    for iter in range(num_epoch):\n",
        "        train_batch,train_label = shuffle(train_batch,train_label)\n",
        "        for batch_size_index in range(0,len(train_batch),batch_size):\n",
        "            current_batch = train_batch[batch_size_index:batch_size_index+batch_size]\n",
        "            sess_results = sess.run(grad_update,feed_dict={x:current_batch,current_iter:iter})\n",
        "            print('Current Iter: ',iter,' Current Train Index: ',batch_size_index,' Current SUM of updated Values: ',np.mean( sum(sess_results[0]) ) ,end='\\r' )\n",
        "        print('\\n-----------------------')\n",
        "\n",
        "    # get the trained map and normalize\n",
        "    # trained_map = sess.run(SOM_layer.getmap()).reshape(map_width_height,map_width_height,3)\n",
        "    # trained_map[:,:,0] = (trained_map[:,:,0]-trained_map[:,:,0].min())/(trained_map[:,:,0].max()-trained_map[:,:,0].min())\n",
        "    # trained_map[:,:,1] = (trained_map[:,:,1]-trained_map[:,:,1].min())/(trained_map[:,:,1].max()-trained_map[:,:,1].min())\n",
        "    # trained_map[:,:,2] = (trained_map[:,:,2]-trained_map[:,:,2].min())/(trained_map[:,:,2].max()-trained_map[:,:,2].min())\n",
        "    # plt.imshow(trained_map)\n",
        "    # plt.show()\n",
        "\n",
        "    # after training is done get the cloest vector\n",
        "    locations = sess.run(SOM_layer.getlocation(),feed_dict={x:train_batch[:batch_size]})\n",
        "    x1 = locations[:,0]; y1 = locations[:,1]\n",
        "    index = [ np.where(r==1)[0][0] for r in train_label ]\n",
        "    index = list(map(str, index))\n",
        "\n",
        "    ## Plots: 1) Train 2) Test+Train ###\n",
        "    plt.figure(1, figsize=(12,6))\n",
        "    plt.subplot(121)\n",
        "    # plt.imshow(trained_map)\n",
        "    plt.scatter(x1,y1)\n",
        "\n",
        "    # Just adding text\n",
        "    for i, m in enumerate(locations):\n",
        "        plt.text( m[0], m[1],index[i], ha='center', va='center', \n",
        "        bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
        "    plt.title('Train MNIST 100')\n",
        "\n",
        "    locations2 = sess.run(SOM_layer.getlocation(),feed_dict={x:test_batch})\n",
        "    x2 = locations2[:,0]; y2 = locations2[:,1]\n",
        "    index2 = [ np.where(r==1)[0][0] for r in test_label ]\n",
        "    index2 = list(map(str, index2))\n",
        "\n",
        "    # Plot 2: Training + Testing\n",
        "    plt.subplot(122)\n",
        "    # plt.imshow(trained_map)\n",
        "    plt.scatter(x1,y1)\n",
        "    plt.scatter(x2,y2)\n",
        "\n",
        "    # Just adding text\n",
        "    for i, m in enumerate(locations):\n",
        "        plt.text( m[0], m[1],index[i], ha='center', va='center', bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
        "\n",
        "    # Just adding text\n",
        "    for i, m in enumerate(locations2):\n",
        "        plt.text( m[0], m[1],index2[i], ha='center', va='center', bbox=dict(facecolor='red', alpha=0.5, lw=0))\n",
        "\n",
        "    plt.title('Test MNIST 10 + Train MNIST 100')\n",
        "    plt.show()\n",
        "\n",
        "# -- end code --"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}