{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "85 SOM c",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YongchangHu/Books/blob/master/85_SOM_c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CvxbD1ndW_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For plotting the images\n",
        "from matplotlib import pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys, os,cv2\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.misc import imread,imresize\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from skimage.transform import resize\n",
        "from imgaug import augmenters as iaa\n",
        "import imgaug as ia\n",
        "from skimage.color import rgba2rgb\n",
        "\n",
        "old_v = tf.logging.get_verbosity()\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "plt.style.use('seaborn-white')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "np.random.seed(6278)\n",
        "tf.set_random_seed(6728)\n",
        "ia.seed(6278)\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "# SOM as layer\n",
        "class SOM_Layer(): \n",
        "\n",
        "    def __init__(self,m,n,dim,learning_rate_som = 0.04,radius_factor = 1.1,gaussian_std=0.5):\n",
        "        \n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.dim = dim\n",
        "        self.gaussian_std = gaussian_std\n",
        "        self.map = tf.Variable(tf.random_normal(shape=[m*n,dim],stddev=0.05))\n",
        "        self.location_vects = tf.constant(np.array(list(self._neuron_locations(m, n))))\n",
        "        self.alpha = learning_rate_som\n",
        "        self.sigma = max(m,n)*1.1\n",
        "\n",
        "    def _neuron_locations(self, m, n):\n",
        "        \"\"\"\n",
        "        Yields one by one the 2-D locations of the individual neurons in the SOM.\n",
        "        \"\"\"\n",
        "        # Nested iterations over both dimensions to generate all 2-D locations in the map\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                yield np.array([i, j])\n",
        "\n",
        "    def getmap(self): return self.map\n",
        "    def getlocation(self): return self.bmu_locs\n",
        "\n",
        "    def feedforward(self,input):\n",
        "    \n",
        "        self.input = input\n",
        "        self.squared_distance = tf.reduce_sum(tf.pow(tf.subtract(tf.expand_dims(self.map, axis=0),tf.expand_dims(self.input, axis=1)), 2), 2)\n",
        "        self.bmu_indices = tf.argmin(self.squared_distance, axis=1)\n",
        "        self.bmu_locs = tf.reshape(tf.gather(self.location_vects, self.bmu_indices), [-1, 2])\n",
        "\n",
        "    def backprop(self,iter,num_epoch):\n",
        "\n",
        "        # Update the weigths \n",
        "        radius = tf.subtract(self.sigma,\n",
        "                                tf.multiply(iter,\n",
        "                                            tf.divide(tf.cast(tf.subtract(self.alpha, 1),tf.float32),\n",
        "                                                    tf.cast(tf.subtract(num_epoch, 1),tf.float32))))\n",
        "\n",
        "        alpha = tf.subtract(self.alpha,\n",
        "                            tf.multiply(iter,\n",
        "                                            tf.divide(tf.cast(tf.subtract(self.alpha, 1),tf.float32),\n",
        "                                                      tf.cast(tf.subtract(num_epoch, 1),tf.float32))))\n",
        "\n",
        "        self.bmu_distance_squares = tf.reduce_sum(\n",
        "                tf.pow(tf.subtract(\n",
        "                    tf.expand_dims(self.location_vects, axis=0),\n",
        "                    tf.expand_dims(self.bmu_locs, axis=1)), 2), \n",
        "            2)\n",
        "\n",
        "        self.neighbourhood_func = tf.exp(tf.divide(tf.negative(tf.cast(\n",
        "                self.bmu_distance_squares, \"float32\")), tf.multiply(\n",
        "                tf.square(tf.multiply(radius, self.gaussian_std)), 2)))\n",
        "\n",
        "        self.learning_rate_op = tf.multiply(self.neighbourhood_func, alpha)\n",
        "        \n",
        "        self.numerator = tf.reduce_sum(\n",
        "            tf.multiply(tf.expand_dims(self.learning_rate_op, axis=-1),\n",
        "            tf.expand_dims(self.input, axis=1)), axis=0)\n",
        "\n",
        "        self.denominator = tf.expand_dims(\n",
        "            tf.reduce_sum(self.learning_rate_op,axis=0) + float(1e-20), axis=-1)\n",
        "\n",
        "        self.new_weights = tf.div(self.numerator, self.denominator)\n",
        "        self.update = tf.assign(self.map, self.new_weights)\n",
        "\n",
        "        return self.update\n",
        "\n",
        "# data\n",
        "PathDicom = \"../../Dataset/cifar-10-batches-py/\"\n",
        "lstFilesDCM = []  # create an empty list\n",
        "for dirName, subdirList, fileList in os.walk(PathDicom):\n",
        "    for filename in fileList:\n",
        "        if not \".html\" in filename.lower() and not  \".meta\" in filename.lower():  # check whether the file's DICOM\n",
        "            lstFilesDCM.append(os.path.join(dirName,filename))\n",
        "\n",
        "# Read the data traind and Test\n",
        "batch0 = unpickle(lstFilesDCM[0])\n",
        "batch1 = unpickle(lstFilesDCM[1])\n",
        "batch2 = unpickle(lstFilesDCM[2])\n",
        "batch3 = unpickle(lstFilesDCM[3])\n",
        "batch4 = unpickle(lstFilesDCM[4])\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=True)\n",
        "train_batch = np.vstack((batch0[b'data'],batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))\n",
        "train_label = np.expand_dims(np.hstack((batch0[b'labels'],batch1[b'labels'],batch2[b'labels'],batch3[b'labels'],batch4[b'labels'])).T,axis=1).astype(np.float32)\n",
        "train_label = onehot_encoder.fit_transform(train_label).toarray().astype(np.float32)\n",
        "\n",
        "test_batch = unpickle(lstFilesDCM[5])[b'data']\n",
        "test_label = np.expand_dims(np.array(unpickle(lstFilesDCM[5])[b'labels']),axis=0).T.astype(np.float32)\n",
        "test_label = onehot_encoder.fit_transform(test_label).toarray().astype(np.float32)\n",
        "\n",
        "# reshape data\n",
        "train_batch = np.reshape(train_batch,(len(train_batch),3,32,32))\n",
        "test_batch = np.reshape(test_batch,(len(test_batch),3,32,32))\n",
        "\n",
        "# rotate data\n",
        "train_batch = np.rot90(np.rot90(train_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float32)\n",
        "test_batch = np.rot90(np.rot90(test_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float32)\n",
        "\n",
        "# hyper parameter 10000\n",
        "num_epoch = 100\n",
        "batch_size = 100\n",
        "\n",
        "learning_rate = 0.000000001\n",
        "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
        "\n",
        "# print out the data shape\n",
        "train_batch = train_batch[:500,:,:,:]\n",
        "train_label = train_label[:500,:]\n",
        "test_batch = test_batch[:batch_size,:,:,:]\n",
        "test_label = test_label[:batch_size,:]\n",
        "\n",
        "print(train_batch.shape)\n",
        "print(train_label.shape)\n",
        "print(test_batch.shape)\n",
        "print(test_label.shape)\n",
        "\n",
        "train_batch = train_batch/255.0\n",
        "test_batch = test_batch/255.0\n",
        "train_batch = np.reshape(train_batch,[500,-1])\n",
        "test_batch = np.reshape(test_batch,[batch_size,-1])\n",
        "\n",
        "# hyper parameter\n",
        "map_width_height  = 30\n",
        "map_dim = 32 * 32 * 3\n",
        "num_epoch = 100\n",
        "batch_size = 100\n",
        "\n",
        "# class\n",
        "SOM_layer = SOM_Layer(map_width_height,map_width_height,map_dim,learning_rate_som=0.8,radius_factor=0.4,gaussian_std = 0.03)\n",
        "\n",
        "# create the graph\n",
        "x = tf.placeholder(shape=[None,map_dim],dtype=tf.float32)\n",
        "current_iter = tf.placeholder(shape=[],dtype=tf.float32)\n",
        "\n",
        "# graph\n",
        "SOM_layer.feedforward(x)\n",
        "map_update=SOM_layer.backprop(current_iter,num_epoch)\n",
        "\n",
        "# session\n",
        "with tf.Session() as sess: \n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # start the training\n",
        "    for iter in range(num_epoch):\n",
        "        for current_train_index in range(0,len(test_batch),batch_size):\n",
        "            currren_train = train_batch[current_train_index:current_train_index+batch_size]\n",
        "            sess_results = sess.run(map_update,feed_dict={x:currren_train,current_iter:iter})\n",
        "            print('Current Iter: ',iter,' Current Train Index: ',current_train_index,' Current SUM of updated Values: ',sess_results.sum(),end='\\r' )\n",
        "        print('\\n-----------------------')\n",
        "\n",
        "    # after training is done get the cloest vector\n",
        "    locations = sess.run(SOM_layer.getlocation(),feed_dict={x:train_batch[:batch_size]})\n",
        "    x1 = locations[:,0]; y1 = locations[:,1]\n",
        "    index = [ np.where(r==1)[0][0] for r in train_label ]\n",
        "    index = list(map(str, index))\n",
        "\n",
        "    ## Plots: 1) Train 2) Test+Train ###\n",
        "    plt.figure(1, figsize=(12,6))\n",
        "    plt.subplot(121)\n",
        "    plt.scatter(x1,y1)\n",
        "    # Just adding text\n",
        "    for i, m in enumerate(locations):\n",
        "        plt.text( m[0], m[1],index[i], ha='center', va='center', \n",
        "        bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
        "    plt.title('Train CIFAR 100')\n",
        "\n",
        "    locations2 = sess.run(SOM_layer.getlocation(),feed_dict={x:test_batch})\n",
        "    locations2 = locations2[:50]\n",
        "    x2 = locations2[:,0]; y2 = locations2[:,1]\n",
        "    index2 = [ np.where(r==1)[0][0] for r in test_label ]\n",
        "    index2 = list(map(str, index2))\n",
        "\n",
        "    plt.subplot(122)\n",
        "    # Plot 2: Training + Testing\n",
        "    plt.scatter(x1,y1)\n",
        "    # Just adding text\n",
        "    for i, m in enumerate(locations):\n",
        "        plt.text( m[0], m[1],index[i], ha='center', va='center', bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
        "\n",
        "    plt.scatter(x2,y2)\n",
        "    # Just adding text\n",
        "    for i, m in enumerate(locations2):\n",
        "        plt.text( m[0], m[1],index2[i], ha='center', va='center', bbox=dict(facecolor='red', alpha=0.5, lw=0))\n",
        "    plt.title('Test CIFAR 50 + Train CIFAR 100')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -- end code --"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}