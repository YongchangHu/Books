{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "85 SOM d",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YongchangHu/Books/blob/master/85_SOM_d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDAqFvWrdYD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys, os,cv2\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.misc import imread,imresize\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from skimage.transform import resize\n",
        "from imgaug import augmenters as iaa\n",
        "import imgaug as ia\n",
        "from skimage.color import rgba2rgb\n",
        "\n",
        "old_v = tf.logging.get_verbosity()\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "plt.style.use('seaborn-white')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "np.random.seed(6278)\n",
        "tf.set_random_seed(6728)\n",
        "ia.seed(6278)\n",
        "\n",
        "def tf_elu(x): return tf.nn.elu(x)\n",
        "def d_tf_elu(x): return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
        "\n",
        "def tf_tanh(x): return tf.nn.tanh(x)\n",
        "def d_tf_tanh(x): return 1 - tf_tanh(x) ** 2\n",
        "\n",
        "def tf_sigmoid(x): return tf.nn.sigmoid(x) \n",
        "def d_tf_sigmoid(x): return tf_sigmoid(x) * (1.0-tf_sigmoid(x))\n",
        "\n",
        "def tf_atan(x): return tf.atan(x)\n",
        "def d_tf_atan(x): return 1.0/(1.0 + x**2)\n",
        "\n",
        "def tf_iden(x): return x\n",
        "def d_tf_iden(x): return 1.0\n",
        "\n",
        "def tf_softmax(x): return tf.nn.softmax(x)\n",
        "\n",
        "# code from: https://github.com/tensorflow/tensorflow/issues/8246\n",
        "def tf_repeat(tensor, repeats):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    input: A Tensor. 1-D or higher.\n",
        "    repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\n",
        "    Returns:\n",
        "    \n",
        "    A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\n",
        "    \"\"\"\n",
        "    expanded_tensor = tf.expand_dims(tensor, -1)\n",
        "    multiples = [1] + repeats\n",
        "    tiled_tensor = tf.tile(expanded_tensor, multiples = multiples)\n",
        "    repeated_tesnor = tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\n",
        "    return repeated_tesnor\n",
        "\n",
        "# ================= VIZ =================\n",
        "# Def: Simple funciton to view the histogram of weights\n",
        "def show_hist_of_weigt(all_weight_list,status='before'):\n",
        "    fig = plt.figure()\n",
        "    weight_index = 0\n",
        "\n",
        "    for i in range(1,1+int(len(all_weight_list)//3)):\n",
        "        ax = fig.add_subplot(1,4,i)\n",
        "        ax.grid(False)\n",
        "        temp_weight_list = all_weight_list[weight_index:weight_index+3]\n",
        "        for temp_index in range(len(temp_weight_list)):\n",
        "            current_flat = temp_weight_list[temp_index].flatten()\n",
        "            ax.hist(current_flat,histtype='step',bins='auto',label=str(temp_index+weight_index))\n",
        "            ax.legend()\n",
        "        ax.set_title('From Layer : '+str(weight_index+1)+' to '+str(weight_index+3))\n",
        "        weight_index = weight_index + 3\n",
        "    plt.savefig('viz/weights_'+str(status)+\"_training.png\")\n",
        "    plt.close('all')\n",
        "\n",
        "# Def: Simple function to show 9 image with different channels\n",
        "def show_9_images(image,layer_num,image_num,channel_increase=3,alpha=None,gt=None,predict=None):\n",
        "    image = (image-image.min())/(image.max()-image.min())\n",
        "    fig = plt.figure()\n",
        "    color_channel = 0\n",
        "    limit = 10\n",
        "    if alpha: limit = len(gt)\n",
        "    for i in range(1,limit):\n",
        "        ax = fig.add_subplot(3,3,i)\n",
        "        ax.grid(False)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        if alpha:\n",
        "            ax.set_title(\"GT: \"+str(gt[i-1])+\" Predict: \"+str(predict[i-1]))\n",
        "        else:\n",
        "            ax.set_title(\"Channel : \" + str(color_channel) + \" : \" + str(color_channel+channel_increase-1))\n",
        "        ax.imshow(np.squeeze(image[:,:,color_channel:color_channel+channel_increase]))\n",
        "        color_channel = color_channel + channel_increase\n",
        "    \n",
        "    if alpha:\n",
        "        plt.savefig('viz/z_'+str(alpha) + \"_alpha_image.png\")\n",
        "    else:\n",
        "        plt.savefig('viz/'+str(layer_num) + \"_layer_\"+str(image_num)+\"_image.png\")\n",
        "    plt.close('all')\n",
        "# ================= VIZ =================\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "# ================= LAYER CLASSES =================\n",
        "class CNN():\n",
        "    \n",
        "    def __init__(self,k,inc,out,act=tf_elu,d_act=d_tf_elu):\n",
        "        self.w = tf.Variable(tf.random_normal([k,k,inc,out],stddev=0.05))\n",
        "        self.m,self.v_prev = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
        "        self.act,self.d_act = act,d_act\n",
        "\n",
        "    def getw(self): return self.w\n",
        "\n",
        "    def feedforward(self,input,stride=1,padding='SAME'):\n",
        "        self.input  = input\n",
        "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA \n",
        "\n",
        "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
        "        grad_part_1 = gradient \n",
        "        grad_part_2 = self.d_act(self.layer) \n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = grad_part_1 * grad_part_2\n",
        "\n",
        "        grad = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = self.w.shape,out_backprop = grad_middle,\n",
        "            strides=[1,stride,stride,1],padding=padding\n",
        "        )\n",
        "\n",
        "        grad_pass = tf.nn.conv2d_backprop_input(input_sizes = [batch_size] + list(grad_part_3.shape[1:]),filter= self.w,out_backprop = grad_middle,\n",
        "            strides=[1,stride,stride,1],padding=padding\n",
        "        )\n",
        "\n",
        "        update_w = []\n",
        "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
        "        update_w.append(tf.assign( self.v_prev,self.v_prev*beta2 + (1-beta2) * (grad ** 2)   ))\n",
        "        m_hat = self.m / (1-beta1)\n",
        "        v_hat = self.v_prev / (1-beta2)\n",
        "        adam_middel = learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
        "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat)  )))         \n",
        "\n",
        "        return grad_pass,update_w \n",
        "\n",
        "class CNN_Trans():\n",
        "    \n",
        "    def __init__(self,k,inc,out,act=tf_elu,d_act=d_tf_elu):\n",
        "        self.w = tf.Variable(tf.random_normal([k,k,inc,out],stddev=0.05))\n",
        "        self.m,self.v_prev = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
        "        self.act,self.d_act = act,d_act\n",
        "\n",
        "    def getw(self): return self.w\n",
        "\n",
        "    def feedforward(self,input,stride=1,padding='SAME'):\n",
        "        self.input  = input\n",
        "        output_shape2 = self.input.shape[2].value * stride\n",
        "        self.layer  = tf.nn.conv2d_transpose(\n",
        "            input,self.w,output_shape=[batch_size,output_shape2,output_shape2,self.w.shape[2].value],\n",
        "            strides=[1,stride,stride,1],padding=padding) \n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA \n",
        "\n",
        "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
        "        grad_part_1 = gradient \n",
        "        grad_part_2 = self.d_act(self.layer) \n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = grad_part_1 * grad_part_2\n",
        "\n",
        "        grad = tf.nn.conv2d_backprop_filter(input = grad_middle,\n",
        "            filter_sizes = self.w.shape,out_backprop = grad_part_3,\n",
        "            strides=[1,stride,stride,1],padding=padding\n",
        "        )\n",
        "\n",
        "        grad_pass = tf.nn.conv2d(\n",
        "            input=grad_middle,filter = self.w,strides=[1,stride,stride,1],padding=padding\n",
        "        )\n",
        "        \n",
        "        update_w = []\n",
        "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
        "        update_w.append(tf.assign( self.v_prev,self.v_prev*beta2 + (1-beta2) * (grad ** 2)   ))\n",
        "        m_hat = self.m / (1-beta1)\n",
        "        v_hat = self.v_prev / (1-beta2)\n",
        "        adam_middel = learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
        "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat)  )))         \n",
        "\n",
        "        return grad_pass,update_w \n",
        "\n",
        "class FNN():\n",
        "    \n",
        "    def __init__(self,input_dim,hidden_dim,act,d_act):\n",
        "        self.w = tf.Variable(tf.random_normal([input_dim,hidden_dim], stddev=0.05))\n",
        "        self.m,self.v_prev = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
        "        self.v_hat_prev = tf.Variable(tf.zeros_like(self.w))\n",
        "        self.act,self.d_act = act,d_act\n",
        "\n",
        "    def feedforward(self,input=None):\n",
        "        self.input = input\n",
        "        self.layer = tf.matmul(input,self.w)\n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "    def backprop(self,gradient=None):\n",
        "        grad_part_1 = gradient \n",
        "        grad_part_2 = self.d_act(self.layer) \n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = grad_part_1 * grad_part_2\n",
        "        grad = tf.matmul(tf.transpose(grad_part_3),grad_middle)\n",
        "        grad_pass = tf.matmul(tf.multiply(grad_part_1,grad_part_2),tf.transpose(self.w))\n",
        "\n",
        "        update_w = []\n",
        "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
        "        update_w.append(tf.assign( self.v_prev,self.v_prev*beta2 + (1-beta2) * (grad ** 2)   ))\n",
        "        m_hat = self.m / (1-beta1)\n",
        "        v_hat = self.v_prev / (1-beta2)\n",
        "        adam_middel = learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
        "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat)  )))     \n",
        "\n",
        "        return grad_pass,update_w   \n",
        "\n",
        "class SOM_Layer(): \n",
        "\n",
        "    def __init__(self,m,n,dim,num_epoch,learning_rate_som ,radius_factor ,gaussian_std):\n",
        "        \n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.dim = dim\n",
        "        self.gaussian_std = gaussian_std\n",
        "        self.num_epoch = num_epoch\n",
        "        self.map = tf.Variable(tf.random_normal(shape=[m*n,dim],stddev=0.05))\n",
        "        self.location_vects = tf.constant(np.array(list(self._neuron_locations(m, n))))\n",
        "        self.alpha = learning_rate_som\n",
        "        self.sigma = max(m,n)*radius_factor\n",
        "\n",
        "    def _neuron_locations(self, m, n):\n",
        "        \"\"\"\n",
        "        Yields one by one the 2-D locations of the individual neurons in the SOM.\n",
        "        \"\"\"\n",
        "        # Nested iterations over both dimensions to generate all 2-D locations in the map\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                yield np.array([i, j])\n",
        "\n",
        "    def getmap(self): return self.map\n",
        "    \n",
        "    def getlocation(self): return self.bmu_locs\n",
        "\n",
        "    def feedforward(self,input):\n",
        "        self.input = input\n",
        "        self.grad_pass = tf.pow(tf.subtract(tf.expand_dims(self.map, axis=0),tf.expand_dims(self.input, axis=1)), 2)\n",
        "        self.squared_distance = tf.reduce_sum(self.grad_pass, 2)\n",
        "        self.bmu_indices = tf.argmin(self.squared_distance, axis=1)\n",
        "        self.bmu_locs = tf.reshape(tf.gather(self.location_vects, self.bmu_indices), [-1, 2])\n",
        "\n",
        "    def backprop(self,iter,num_epoch):\n",
        "\n",
        "        # Update the weigths \n",
        "        radius = tf.subtract(self.sigma,\n",
        "                                tf.multiply(iter,\n",
        "                                            tf.divide(tf.cast(tf.subtract(self.alpha, 1),tf.float32),\n",
        "                                                    tf.cast(tf.subtract(num_epoch, 1),tf.float32))))\n",
        "\n",
        "        alpha = tf.subtract(self.alpha,\n",
        "                            tf.multiply(iter,\n",
        "                                            tf.divide(tf.cast(tf.subtract(self.alpha, 1),tf.float32),\n",
        "                                                      tf.cast(tf.subtract(num_epoch, 1),tf.float32))))\n",
        "\n",
        "        self.bmu_distance_squares = tf.reduce_sum(\n",
        "                tf.pow(tf.subtract(\n",
        "                    tf.expand_dims(self.location_vects, axis=0),\n",
        "                    tf.expand_dims(self.bmu_locs, axis=1)), 2), \n",
        "            2)\n",
        "\n",
        "        self.neighbourhood_func = tf.exp(tf.divide(tf.negative(tf.cast(\n",
        "                self.bmu_distance_squares, \"float32\")), tf.multiply(\n",
        "                tf.square(tf.multiply(radius, self.gaussian_std)), 2)))\n",
        "\n",
        "        self.learning_rate_op = tf.multiply(self.neighbourhood_func, alpha)\n",
        "        \n",
        "        self.numerator = tf.reduce_sum(\n",
        "            tf.multiply(tf.expand_dims(self.learning_rate_op, axis=-1),\n",
        "            tf.expand_dims(self.input, axis=1)), axis=0)\n",
        "\n",
        "        self.denominator = tf.expand_dims(\n",
        "            tf.reduce_sum(self.learning_rate_op,axis=0) + float(1e-20), axis=-1)\n",
        "\n",
        "        self.new_weights = tf.div(self.numerator, self.denominator)\n",
        "        self.update = [tf.assign(self.map, self.new_weights)]\n",
        "\n",
        "        return self.update,tf.reduce_mean(self.grad_pass,1)\n",
        "\n",
        "# ================= LAYER CLASSES =================\n",
        "\n",
        "# data\n",
        "PathDicom = \"../../Dataset/cifar-10-batches-py/\"\n",
        "lstFilesDCM = []  # create an empty list\n",
        "for dirName, subdirList, fileList in os.walk(PathDicom):\n",
        "    for filename in fileList:\n",
        "        if not \".html\" in filename.lower() and not  \".meta\" in filename.lower():  # check whether the file's DICOM\n",
        "            lstFilesDCM.append(os.path.join(dirName,filename))\n",
        "\n",
        "# Read the data traind and Test\n",
        "batch0 = unpickle(lstFilesDCM[0])\n",
        "batch1 = unpickle(lstFilesDCM[1])\n",
        "batch2 = unpickle(lstFilesDCM[2])\n",
        "batch3 = unpickle(lstFilesDCM[3])\n",
        "batch4 = unpickle(lstFilesDCM[4])\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=True)\n",
        "train_batch = np.vstack((batch0[b'data'],batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))\n",
        "train_label = np.expand_dims(np.hstack((batch0[b'labels'],batch1[b'labels'],batch2[b'labels'],batch3[b'labels'],batch4[b'labels'])).T,axis=1).astype(np.float32)\n",
        "train_label = onehot_encoder.fit_transform(train_label).toarray().astype(np.float32)\n",
        "\n",
        "test_batch = unpickle(lstFilesDCM[5])[b'data']\n",
        "test_label = np.expand_dims(np.array(unpickle(lstFilesDCM[5])[b'labels']),axis=0).T.astype(np.float32)\n",
        "test_label = onehot_encoder.fit_transform(test_label).toarray().astype(np.float32)\n",
        "\n",
        "# reshape data\n",
        "train_batch = np.reshape(train_batch,(len(train_batch),3,32,32))\n",
        "test_batch = np.reshape(test_batch,(len(test_batch),3,32,32))\n",
        "\n",
        "# rotate data\n",
        "train_batch = np.rot90(np.rot90(train_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float32)\n",
        "test_batch = np.rot90(np.rot90(test_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float32)\n",
        "\n",
        "# hyper parameter 10000\n",
        "num_epoch = 100\n",
        "batch_size = 100\n",
        "\n",
        "map_width_height  = 30\n",
        "map_dim = 256\n",
        "\n",
        "learning_rate = 0.00000001\n",
        "beta1,beta2,adam_e = 0.9,0.999,1e-8\n",
        "\n",
        "# print out the data shape\n",
        "train_batch = train_batch[:500,:,:,:]\n",
        "train_label = train_label[:500,:]\n",
        "test_batch = test_batch[:batch_size,:,:,:]\n",
        "test_label = test_label[:batch_size,:]\n",
        "\n",
        "print(train_batch.shape)\n",
        "print(train_label.shape)\n",
        "print(test_batch.shape)\n",
        "print(test_label.shape)\n",
        "\n",
        "train_batch = train_batch/255.0\n",
        "test_batch = test_batch/255.0\n",
        "\n",
        "# define class here\n",
        "el1 = CNN(3,3,32)\n",
        "el2 = CNN(3,32,64)\n",
        "el3 = FNN(8*8*64,2048,act=tf_atan,d_act=d_tf_atan)\n",
        "el4 = FNN(2048,256,act=tf_atan,d_act=d_tf_atan)\n",
        "SOM_layer = SOM_Layer(map_width_height,map_width_height,map_dim,num_epoch = num_epoch,learning_rate_som=0.8,radius_factor=2.8,gaussian_std = 0.03)\n",
        "\n",
        "# graph\n",
        "x = tf.placeholder(shape=[None,32,32,3],dtype=tf.float32,name=\"input\")\n",
        "current_iter = tf.placeholder(shape=[],dtype=tf.float32)\n",
        "\n",
        "# encoder\n",
        "elayer1 = el1.feedforward(x,padding='SAME',stride=2)\n",
        "elayer2 = el2.feedforward(elayer1,padding='SAME',stride=2)\n",
        "elayer3_flatten = tf.reshape(elayer2,[batch_size,-1])\n",
        "elayer3 = el3.feedforward(elayer3_flatten)  \n",
        "elayer4 = el4.feedforward(elayer3)  \n",
        "SOM_layer.feedforward(elayer4)\n",
        "\n",
        "SOM_Update,SOM_grad = SOM_layer.backprop(current_iter,num_epoch)\n",
        "egrad4,egrad4_up = el4.backprop(SOM_grad)\n",
        "egrad3,egrad3_up = el3.backprop(egrad4)\n",
        "egrad2_Input = tf.reshape(egrad3,[batch_size,8,8,64])\n",
        "egrad2,egrad2_up = el2.backprop(egrad2_Input,padding='SAME',stride=2)\n",
        "egrad1,egrad1_up = el1.backprop(egrad2,padding='SAME',stride=2)\n",
        "grad_update = SOM_Update +egrad4_up +egrad3_up + egrad2_up + egrad1_up\n",
        "\n",
        "# sess\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # start the training\n",
        "    for iter in range(num_epoch):\n",
        "\n",
        "        train_batch,train_label = shuffle(train_batch,train_label)\n",
        "        for batch_size_index in range(0,len(train_batch),batch_size):\n",
        "            current_batch = train_batch[batch_size_index:batch_size_index+batch_size]\n",
        "            sess_results = sess.run(grad_update,feed_dict={x:current_batch,current_iter:iter})\n",
        "            print('Current Iter: ',iter,' Current Train Index: ',batch_size_index,\n",
        "            ' Current SUM of updated Values: ',sess_results[0].sum(),end='\\r' )\n",
        "        print('\\n-----------------------')\n",
        "\n",
        "    # after training is done get the cloest vector\n",
        "    locations = sess.run(SOM_layer.getlocation(),feed_dict={x:train_batch[:batch_size]})\n",
        "    x1 = locations[:,0]; y1 = locations[:,1]\n",
        "    index = [ np.where(r==1)[0][0] for r in train_label ]\n",
        "    index = list(map(str, index))\n",
        "\n",
        "    ## Plots: 1) Train 2) Test+Train ###\n",
        "    plt.figure(1, figsize=(12,6))\n",
        "    plt.subplot(121)\n",
        "    plt.scatter(x1,y1)\n",
        "    # Just adding text\n",
        "    for i, m in enumerate(locations):\n",
        "        plt.text( m[0], m[1],index[i], ha='center', va='center', \n",
        "        bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
        "    plt.title('Train CIFAR 100')\n",
        "\n",
        "    locations2 = sess.run(SOM_layer.getlocation(),feed_dict={x:test_batch})\n",
        "    locations2 = locations2[:50]\n",
        "    x2 = locations2[:,0]; y2 = locations2[:,1]\n",
        "    index2 = [ np.where(r==1)[0][0] for r in test_label ]\n",
        "    index2 = list(map(str, index2))\n",
        "\n",
        "    plt.subplot(122)\n",
        "    # Plot 2: Training + Testing\n",
        "    plt.scatter(x1,y1)\n",
        "    # Just adding text\n",
        "    for i, m in enumerate(locations):\n",
        "        plt.text( m[0], m[1],index[i], ha='center', va='center', bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
        "\n",
        "    plt.scatter(x2,y2)\n",
        "    # Just adding text\n",
        "    for i, m in enumerate(locations2):\n",
        "        plt.text( m[0], m[1],index2[i], ha='center', va='center', bbox=dict(facecolor='red', alpha=0.5, lw=0))\n",
        "    plt.title('Test CIFAR 50 + Train CIFAR 100')\n",
        "    plt.show()\n",
        "\n",
        "# -- end code -"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}